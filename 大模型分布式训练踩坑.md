## 环境迁移配置
1. 使用tar直接打包如：anaconda3/envs/llm路径下的环境，然后直接到这边对应anaconda3/envs/下解压
2. conda activate llm
3. python -m pip uninstall llamafactory
4. 根据Qwen的指引重新安装：
    1. git clone --depth 1 [https://github.com/hiyouga/LLaMA-Factory.git](https://github.com/hiyouga/LLaMA-Factory.git)（本地下载后上传）
    2. cd LLaMA-Factory
    3. pip install -e .  --no-build-isolation  # 跳过依赖包安装，不然会报错，因为没有网
5. 可能要修改一部分包的路径(例如torchrun)：  
此时需要  vim anaconda3/envs/llm/bin/torchrun，  
把第一行的python路径如#!****/anaconda2/envs/llm/bin/python3.11改成llm环境的python路径即可  
llm环境的路径可以conda activate llm后执行which python得到



## 模型训练
### 单机训练
1. 参数：使用全量模型微调，deepspeed 推荐选择stage2，若不选显存应该会爆，stage 3目前报错
2. 参数：preprocessing_num_works默认为16，可以尽量选大，A800服务器是256个核，可设置为128，不然处理数据会占据相当部分时间，若使用webui训练，此参数可以在“其他参数设置”栏中手动添加

### 多机多卡
[https://llamafactory.readthedocs.io/zh-cn/latest/advanced/distributed.html#id9](https://llamafactory.readthedocs.io/zh-cn/latest/advanced/distributed.html#id9)



以下命令是**未启用NCCL通讯模块**，能跑起来，但速度很慢。。。

主机运行：  
**<font style="color:#DF2A3F;">NCCL_IB_DISABLE=1</font>** FORCE_TORCHRUN=1 NNODES=2 **NODE_RANK=0** MASTER_ADDR=192.168.20.14 MASTER_PORT=29500 \  
llamafactory-cli train config/train_config.yaml  


副机运行：  
**<font style="color:#DF2A3F;">NCCL_IB_DISABLE=1</font>** FORCE_TORCHRUN=1 NNODES=2 **NODE_RANK=1** MASTER_ADDR=192.168.20.14 MASTER_PORT=29500 \  
llamafactory-cli train config/train_config.yaml

+ conda环境可以直接使用远程拷贝 scp ip:路径 路径搭建
+ 数据和模型，建议直接将主机的路径挂载到副机的路径下，实现共享路径  
执行： ` sudo mount 192.169.20.14:/你的路径/data /你的路径/data`



**启用NCCL通讯**

+ sudo apt install ntp



FORCE_TORCHRUN=1 NNODES=2 **NODE_RANK=0** MASTER_ADDR=192.168.20.14 MASTER_PORT=29500 \  
llamafactory-cli train config/train_config.yaml  


副机运行：  
FORCE_TORCHRUN=1 NNODES=2 **NODE_RANK=1** MASTER_ADDR=192.168.20.14 MASTER_PORT=29500 \  
llamafactory-cli train config/train_config.yaml



问题：ibstat 状态为initializing  
解决：重启交换机 systemctl restart opensmd                systemctl enable opensmd



以下是Deepspeed的三种策略和NCCL的原理、特点和注意事项的中文版本，您可以将其直接整合到您的文档中：

---

### **Deepspeed的三种策略：**
#### 1**Stage 1: ZeRO优化（数据并行）**
+ **原理**: Stage 1主要通过将模型的参数和梯度划分到不同的设备上来优化内存。这种技术通过减少内存占用来使得更大的模型可以在较少的内存下进行训练。
+ **特点**:
    - 使用**数据并行**，将模型的参数分割到不同的GPU上。
    - 通过在多个设备上存储参数来减少内存占用。
    - 对于模型过大无法在单个GPU内存中训练的情况，这是一个比较简单的优化策略。
+ **注意事项**:
    - **网络带宽**: 数据并行要求快速的GPU间通信，确保网络带宽足够高，能够处理数据传输。
    - **内存使用**: 尽管内存占用减少，但对于更大的模型，内存还是有可能达到上限。

#### **Stage 2: ZeRO-2（优化器状态分割）**
+ **原理**: Stage 2在Stage 1的基础上，通过将优化器状态（如动量和方差）也分割到多个GPU上，进一步减少内存占用。
+ **特点**:
    - **显著减少内存使用**，通过分布式存储优化器状态，允许训练更大的模型。
    - 对于具有非常大参数数量的模型效果显著。
    - 需要高效的**通信策略**，因为优化器状态在多个GPU之间需要同步。
+ **注意事项**:
    - **GPU间通信**: 比Stage 1更为关键，因为优化器状态需要同步。
    - 需要调整**批处理大小**和**梯度累积步数**，以避免内存溢出错误。
    - 如果通信层没有优化，Stage 2可能仍会出现瓶颈。

#### **Stage 3: ZeRO-3（全模型并行）**
+ **原理**: Stage 3在Stage 2的基础上，将整个模型（包括参数、梯度和优化器状态）分割到多个设备上，实现模型并行，允许训练更大的模型，减少内存占用。
+ **特点**:
    - **内存效率**: 通过完全分割数据和模型参数，帮助训练非常大的模型，超出单个GPU的内存限制。
    - 提供了**最激进的优化**，使得可以训练比所有GPU内存总和还大的模型。
+ **注意事项**:
    - **通信开销**: 在此阶段，GPU间的通信开销可能变得显著。需要高带宽的互联（如NVLink、InfiniBand）来保证高效的数据交换。
    - **调试复杂度**: 由于完全的模型并行，调试过程可能会更加复杂，且训练过程的速度可能比Stage 2更慢，因为需要更多的同步。

---
![image](https://github.com/user-attachments/assets/05e910e9-cefd-4003-855a-999e4813f237)

### **NCCL的原理/特点/注意事项**
#### **原理**
NCCL（NVIDIA Collective Communication Library）是一个高性能的库，专门用于多GPU之间的集体通信。它针对深度学习工作负载，尤其是在单机和多机环境下的多GPU通信进行了优化。

+ **基本操作**:
    - **广播** (broadcast): 将数据从一个源发送到多个接收端。
    - **聚合** (reduce): 将多个源的数据聚合成一个。
    - **同步** (all-reduce): 通过集体方式进行数据的交换与聚合。
    - **点对点通信** (send/recv): GPU之间的直接通信。

#### **特点**
+ **高效性**: NCCL在高吞吐量和低延迟方面进行了优化，特别适合具有大量GPU的系统。
+ **灵活性**: 支持多种集体操作，如all-reduce、broadcast和gather，并且支持不同的通信后端（如PCIe、NVLink、InfiniBand）。
+ **可扩展性**: 设计上可以从单节点少量GPU扩展到大规模多节点系统，保持高效的通信。

#### **注意事项**
+ **网络设置**: NCCL依赖于快速的GPU间和节点间通信（如InfiniBand或NVLink）来实现性能。如果网络性能不佳，可能成为瓶颈。
+ **同步问题**: 使用NCCL进行分布式训练时，需要注意进程的同步，否则可能出现通信延迟。确保有合适的屏障机制来同步各个进程。
+ **容错性**: 在多节点部署时，网络故障可能导致训练中断。建议实现错误处理和恢复策略。
+ **NCCL版本**: 不同版本的NCCL可能具有不同的性能优化或错误，建议使用最新的稳定版本。
+ **环境配置**: 配置环境变量如`NCCL_DEBUG=INFO`，可以帮助诊断通信问题，尤其是遇到慢速或失败的情况时。

